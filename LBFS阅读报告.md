# **LBFS阅读报告**

#### 摘要

1. 传统网络文件系统在慢速网络上消耗带宽太高，性能无法接受。
2. LBFS利用文件之间的或者同一文件版本之间的相似性来节省带宽，当服务器和客户机的文件数据相同时，它就不通过网络发送数据。

#### 介绍

1. 目前两种主流方法访问远程数据：
   * 创建文件的本地副本（有更新冲突的风险）
   * 通过交互应用程序远程控制机器操作文件（占用带宽太高，响应很慢）
2. NFS（Network File System）：
   * 紧密的一致性
   * 通过交互应用程序和文件系统访问远程数据
3. 常见文件系统的问题：产生额外的文件（比如自动保存的副本），在网络慢的情况下，这个行为会阻塞编辑器，延迟用户打字或者消耗大量资源。并且还会通过网络传输此类文件的全部内容。（LBFS传输的仅是不同的数据，少于应用程序写入的数据，减少等待文件I/O的时间）
4. LBFS利用跨文件的相似性：
   * 反面例子：RCS版本控制系统使用的临时文件、后记文件和文字处理文档在相邻的版本之间具有很多相似性，这些文件的复制会导致内容的大量重复。
   * 实现原理：先将文件划分为若干块，然后进行根据HASH值对这些块进行索引，LBFS识别出客户端和服务器中都已经存在的数据块（也就是没有修改过的数据块），传输时就不传这些块，避免网络传输冗余的数据。
5. LBFS还可以结合其他文件系统，其他系统通过降低文件一致性的标准（当然不能低于公认一致性的底线），这样可以使得LBFS传输的数据更少，甚至在间歇性的网络状况下都能工作。

#### 相关工作

1. 几个概念：
   * 回调：服务器记住哪些客户端读取了数据，当服务器要对数据进行修改的时候，就通知这些客户端之前读取的那些数据已经失效。
     * 缺点：记住所有读取数据的客户端对服务器来说是一个较大的负担，而且，一旦有客户端和服务器失去连接，服务器就不能进行修改操作。
   * 轮询：客户端每次读取数据时都要询问服务器数据是否是最新的，如果不是最新的话就从服务器传回最新的数据。
     * 缺点：每次读取数据都要和服务器通信确认，服务器负载很大。
   * 租约：一种协议，该协议赋予持有该协议者在某个属性的特定的有限时间的权利。即服务器给予客户端在一定期限内可以控制修改操作的权力。期限为0就是轮询，期限为无限长就是回调。
2. 其他文件系统的机制：
   * AFS：服务器通过回调通知客户端，用户直接访问客户端的缓存文件即可，不需要网络流量。
   * NFS4：批处理文件系统操作，减少网络往返。
   * Echo：采用后写操作。
   * JetFile：将最后一个写文件的客户端作为服务器，并且将它的文件内容传输到下一个读取的客户端。
   * CODA：将用户对文件的修改记录在客户端，后台发送给服务器，但是一致性的保证较弱，需要用户手动解决更新的冲突。
3. LBFS使用大型缓存和租约减少服务器和客户端之间连接的负载，并且避免客户端断网或崩溃的问题。

#### 设计

1. LBFS由于是传输不同的数据块，所以如果应用程序对数据块进行加密的话，它就会传输整个文件，减少不了带宽消耗。

##### 索引

1. 哈希算法采用SHA-1算法，不同的输入产生相同输出的概率十分低，所以服务端和客户端的数据块的哈希值相同的话，那么它们就是同一块数据块。
2. 主要挑战：
   * 块大小的划分：太大会增加传输负担，传送冗余的数据；太小的话哈希值会很多（个人理解）
   * 边界的确定：如果按8KB的偏移量划分的话，在文件头部插入数据时，所有块的划分都会被打乱，哈希值也是，无法比较。
3. Rsync算法：A（客户端）向B（服务器）传文件时，先通过文件名确定文件，然后B将文件分成固定大小的块(不重叠，例如1-8，9-16等)进行哈希，将哈希值发给A，A再对自己的这个文件进行哈希（A这边是重叠的，如1-8，2-9等，这样保证肯定会覆盖到），如果A中某个块的哈希值和B发送的某个哈希值相同的话，A就不发送这个数据块了。
   * 问题1：通过文件名确定文件太过于理想化了，必须还得通过如边界相似度估算技术（估算大小确定是否为该文件？）确定文件。
   * 问题2：具有相同名称的目标文件的静态库（AR），比如有四个文件foo/a.cpp; foo/a.h; bar/a.cpp; bar/a.h，将两个文件夹下的文件都编译成a.o，加入静态库时，后面的a.o会覆盖前面的a.o，所以考虑到这种重复的可能，还应通过其他的文件数据块进行验证和重构文件（不然无法确定a.o到底是哪个a.o）。

###### LBFS解决文件名重复的方案

1. 仅考虑不重叠的数据块，而且边界的划分是依据文件的内容，而不是文件内的位置，这就使得插入和删除操作容易很多。

2. 文件的边界使用拉宾指纹（检测和比较重复的数据）标记，使用滑动窗口（大小为48bytes）进行计算，当数据的低13位等于选定值时，就是断点，即边界。

3. 分块举例：

   ![image-20201214001434367](.\assets\分块.png)

   * a是原始文件的数据块的划分。
   * b是在a的基础上，在c<sub>4</sub>块中添加一部分变成新块c<sub>8</sub>
   * c是在b的基础上，在c<sub>5</sub>中添加一个断点，将其变成新块c<sub>9</sub>、c<sub>10</sub>
   * d是在c的基础上，将c<sub>2</sub>和c<sub>3</sub>中的断点删除，合并成新块c<sub>11</sub>

###### 病理病例（我也不知道啥意思，应该是问题和缺陷？- -！）

1. 如果划分结果有很多的数据块大小等于断点窗口大小（48bytes），那么索引就和文件的大小一样，等于白干。
2. 如果划分的块太大的话，就和传输文件没啥区别，使用拉宾指纹会有一个缺陷：在很多数据为0时它是不会产生断点的。（这边也没咋理解- -！）
3. 解决方案：
   * 规定最小块：最小块大小为2K，断点后的2K范围内不允许产生新的断点。
   * 规定最大块：最大快大小为64K，如果超过了64K还没有产生断点，就人为地产生断点(这种做法可能会破坏文件版本之间文件块的同步)
   * 如果发生了，上面所说的问题，理论上是会使性能大大降低，但是由于问题的特殊性（这种情况下的数据一般都是长时间的0或者重复的序列），按照常规方式压缩这些文件，就能解决这些问题。

###### 块数据库

1. LBFS采用数据库来识别和定位重复的块，将64位的密匙映射到文件，偏移量。修改文件后，就更新这些映射。有几个问题：
   * 如果服务器端修改了LBFS导出的文件，那么此时数据就和数据库的映射不一致了。
   * 如果客户端意外崩溃，磁盘缓存的内容会损坏。
2. 解决方案：
   * 不依赖数据库的正确性和完整性。LBFS每次都对所有文件块进行重新SHA-1的哈希计算，并且和数据库比对更新。这样也解决了崩溃恢复的问题，因为每次都是重新计算。

##### 协议

1. LBFS协议基于NFS3，NFS通过不透明句柄命名文件，句柄可以特定的偏移量读取和写入数据，LBFS在此基础上添加扩展，来利用文件间的公共性。
2. LBFS向协议添加租约来保存最近访问文件的往返行程。
3. LBFS采用积极的RPC调用流水线来容忍网络延迟。
4. LBFS采用常规的gzip来压缩RPC通信。

###### 文件一致性

1. 确定文件时最新版本的方法：
   * 获取读取的租约，如果租约未过期并且和高速缓存中的版本一致，就立即打开，不会与服务器端产生联系。
   * 如果租约过期，先向服务器询问文件属性（同时也获取租约），如果获得的属性中的修改时间和inode的更改时间与告诉缓存中的一致，就使用告诉缓存中的文件版本，否则就进一步联系服务器，传输最新的文件。
2. LBFS只保证打开的一致性（打开的文件是最新版本），所以LBFS不需要写的租约，在关闭文件之后，后台自动提交修改。所以，当有多个客户端进行写操作时，最后一个提交客户端会覆盖之前的所有修改。

###### 文件读取

1. LBFS的文件读取使用的时一种RPC的过程获取哈希，而不是NFS的GETHASH协议。

2. GETHASH返回SHA-1的哈希值，不返回文件数据。

3. 读取流程：

   ![image-20201214014038497](.\assets\读取流程.png)

   * 客户端先向服务器发送获取哈希值的请求；
   * 服务器计算哈希值，返回给客户端；
   * 客户端发现sha1，sha2不在块数据库的映射里，就向服务器发起读取这两块的请求；
   * 服务器返回与sha1和sha2的哈希值有关的数据；
   * 客户端接收并将其放入块数据库。

###### 文件写入

1. LBFS和NFS的写入文件方式不同。

   * NFS的写入文件方式是每次写入时都会更新服务器上的文件；
   * LBFS的写入文件方式是当客户端关闭文件时，一次性对服务器上的文件进行修改。

2. LBFS写入方式的好处：

   * 可以利用先前版本和当前版本的共通性；
   * 可以避免文件一些奇怪的中间状态（比如一个ASCII文件可能暂时有许多0的块）
   * 由于最后的一次性更新是原子性的，所以即使同时写入时，前一个的修改会被覆盖，但起码保证了文件和最后写入的客户端的版本一致，而不会出现文件“混搭”的场面。

3. 更新原子性的保证：

   * 服务器先创建一个临时文件，先写入这个临时文件，再更新实际的文件。（临时文件更新的时候也尽量使用原本的文件来节省带宽）

   * 实现该更新协议的四个RPC：MKTMPFILE（创建临时文件）、TMPWRITE（客户端更新创建的临时文件）、CONDWRITE（和TMPWRITE类似，但是传入的参数是哈希值，在本地寻找哈希值指定的数据，找到后写入临时文件）、COMMITTMP（将临时文件写入实际文件）。

   * 写入流程：

     ![image-20201214021828415](.\assets\写入流程.png)

     * 客户端关闭文件后先向服务器发送创建临时文件的RPC和CONDWRITE RPC；
     * 服务器创建临时文件，并在本地寻找哈希值相同的数据块，sha2没找到就返回未找到的标识符；
     * 客户端知道服务器需要sha2，就将该数据块发送给服务器；
     * 服务器接收成功后，再将临时文件写入实际文件并告诉客户端更新成功。

###### 安全性

1. LBFS使用来自SFS的安全基础架构。
2. 每个服务器上都有一个公共密匙，并且在路径名中嵌入密匙，并且将LBFS集成到SFS的自动挂载系统。
3. 服务器像用户进行身份验证，用户向客户端进行身份验证。
4. 所有操作都采用公共密钥术。
5. LBFS可能会泄露不允许用户读取的信息（用户通过CONDWRITE来推断哈希值）

#### 实现

1. 客户端使用xfs实现文件系统，服务器通过NFS访问文件，客户端和服务器使用Sun RPC通过TCP进行通信。LBFS对服务器和客户端之间的流量出了支持身份验证和加密，还支持压缩。架构图如下：

   ![image-20201214110527840](.\assets\架构.png)

##### 块索引

1. 客户端和服务器都会维护块索引，服务器为文件系统的内容建立块索引，客户端则维护本地的缓存。

##### 服务器端实现

1. LBFS服务器通过假装为NFS客户端来访问文件系统，将LBFS请求转换为NFS。
2. LBFS不需要实现访问控制，服务器仅将请求映射到用户ID即可，让NFS服务器确定是否授予访问权限。
3. 重命名文件是，由于NFS的文件句柄不变，所以服务器不需要更新块索引。
4. 服务器会在根目录下创建一个回收站，来存放创建的临时文件，这些文件并不会被立即删除，而是当服务器需要更多空间的时候才去删除这些文件。

###### 静态i-number问题

1. 文件被覆盖时，文件的i-number不会改变，所以临时文件必须真正地将文件内容写入实际文件，而不是仅仅将临时文件重命名后覆盖实际文件。
2. 步骤1这样做效率低下，而且复制文件内容的时候，客户端还不能访问文件。
3. 当服务器崩溃的时候，文件就会出于不一致的状态。
4. LBFS会截断文件，并利用某些截断文件和先前的文件内容的相似性重构文件，但是NFS不允许在不更改截断文件的i-number的情况下进行这些操作。所以，LBFS延迟临时文件的删除，在回收站中创建副本，以便来构建新版本。
5. 解决方案：将A文件截断成0长度，然后将文件B的内容原子替换为A先前的内容。但是LBFS并没有实现这样的操作。

##### 客户端实现

1. LBFS使用xfs设备驱动程序，xfs允许用户级程序通过/dev中的设备节点将消息传递到内核，从而实现文件系统。
2. 每当需要文件内容或者将其写回服务器时，驱动程序就会通知LBFS客户端。此时LBFS客户端就会请求获取远程文件到缓存，并且将已打开的文件和缓存中的文件进行绑定。xfs即可直接从缓存中满足读取和写入请求。

#### 实际表现

##### 文件中的重复数据

1. emacs在软件开发工作负载下文件的共同点（可以看到这些重复率还是很高的）：

![image-20201214122819168](.\assets\emacs文件表.png)

2. LBFS适用于后记文档预览，比如调整方程式等，不适合文档的大量修改，除非页面按章编号。

3. 分块算法的实际效果：

![image-20201214123910800](.\assets\实际效果.png)

* 总数据大小为354MB，分成了42,466块；

* 生成的数据库消耗了4.7MB；
* 中位数为5.8K，平均数为8,570字节，接近预期的8,240字节；
* 由于最大最小块的限制，抑制了11,379个断点，插入了75个断点；
* 最后的有小于2K的块，因为有小于2K的文件和正好碰到了文件边界。
* 分块的通用性：![image-20201214124708977](.\assets\分块通用性结果.png)，虽然小块会有更强的通用性，但是，哈希和传输的成本也增加了，所以窗口大小对通用性的影响不大。

##### 实际工作负载

1. 三个工作负载：

   * 打开一个1.4M的word文档；
   * 用gcc重新编译emacs 20.7；
   * 对perl 5.6.0源代码树进行修改，将其转换为perl 5.6.1

2. 三个工作负载的实际表现（客户端写入服务器和从服务端读取数据的表现，后面还有每个文件系统表现的具体的原因分析，就不放出来了）：

   ![image-20201214135548755](.\assets\负载实际表现.png)

#### 总结

1. 在编辑文档和编译软件之类的常见操作下，LBFS可以比传统文件系统消耗少一个数量级的带宽，并且可以替代远程的交互式程序去对文件进行操作。
